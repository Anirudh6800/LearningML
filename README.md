# LearningML

This repository is a hands-on journey into the foundational concepts of Machine Learning. It covers essential components of building neural networks from scratch using Python, without relying on high-level libraries like TensorFlow or PyTorch. Ideal for beginners and those wanting to solidify their understanding of the core mathematics and logic behind ML models.

### üîç Contents

- **Activation functions** ‚Äì Implementations and usage of common activation functions like ReLU, Sigmoid, and Tanh.
- **Broadcasting and array summation** ‚Äì Demos of NumPy broadcasting and vectorized operations.
- **Classes & Neurons** ‚Äì Object-oriented approach to modeling perceptrons and basic neuron logic.
- **Cross Entropy Loss** ‚Äì Manual implementation of the cross-entropy loss function.
- **Dense Layer Implementation** ‚Äì Custom class to simulate a dense (fully connected) layer.
- **Matrix Multiplication** ‚Äì Basic matrix operations relevant to neural networks.
- **One-Hot Encoding** ‚Äì Manual one-hot encoding of categorical labels.
- **Random Classifiers** ‚Äì Basic classifiers built without training to demonstrate classification logic.

### üß† Goal

The main goal is to **build intuition** by implementing ML components from the ground up. This repo complements theoretical study by enabling practical experimentation and better understanding of the inner workings of machine learning models.

